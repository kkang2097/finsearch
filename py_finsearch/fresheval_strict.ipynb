{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cIMEmj9Esik",
    "outputId": "b8953f3b-8ff6-42b5-ff15-af0aaf05406d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.9)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "#@title Installing required Python packages\n",
    "\n",
    "\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "wW3P9qyxHHU8"
   },
   "outputs": [],
   "source": [
    "#@title Importing Python libraries and modules\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import pytz\n",
    "import dateutil\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from openai import OpenAI\n",
    "import tabulate\n",
    "import textwrap\n",
    "\n",
    "\n",
    "current_date = datetime.datetime.now(\n",
    "        pytz.timezone(\"America/Los_Angeles\")\n",
    "    ).strftime(\"%B %d, %Y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "98YRRHnz0SGu"
   },
   "outputs": [],
   "source": [
    "#@title API keys\n",
    "\n",
    "\n",
    "# OpenAI's API key (sign up at https://platform.openai.com/signup to get $5 in\n",
    "# free credit that can be used during your first 3 months)\n",
    "openai_api_key = \"something\"  # @param {type:\"string\"}\n",
    "openai_client = OpenAI(\n",
    "  api_key=openai_api_key,\n",
    "  base_url=\"localhost:7878\"\n",
    ")\n",
    "\n",
    "assert (\n",
    "      openai_api_key is not None and openai_api_key != ''\n",
    "  ), \"OpenAI's API key is not set\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "7x4S8-FHHtK1"
   },
   "outputs": [],
   "source": [
    "#@title Function calling for the base LLM\n",
    "\n",
    "\n",
    "def call_llm_api(prompt, model, temperature, max_tokens, chat_completions=True):\n",
    "  # See https://platform.openai.com/docs/guides/gpt for details\n",
    "  if chat_completions:\n",
    "    # Chat completions API\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a helpful assistant. Respond as concisely as\"\n",
    "                    f\" possible. Knowledge cutoff: {current_date}.\"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"What's today's date?\"},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"Today is {current_date} in Pacific Standard Time.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "  else:\n",
    "    # Completions API\n",
    "    response = openai_client.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        prompt=prompt,\n",
    "    )\n",
    "    return response.choices[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xmQZYfPD3sxL"
   },
   "outputs": [],
   "source": [
    "#@title Instructions & demonstration examples\n",
    "\n",
    "\n",
    "prefix = (\n",
    "    \"Please evaluate the response to a question under strict evaluation, where\"\n",
    "    \" no hallucinations, outdated information, or ill-formed answers are\"\n",
    "    \" allowed. Please credit the response only if it provides a confident and\"\n",
    "    \" definitive answer, or the correct answer can be obviously inferred from\"\n",
    "    \" the response. The primary or final answer when standing alone must be\"\n",
    "    \" accurate. Any additional information that is provided must not contradict\"\n",
    "    \" the primary answer or reshape one's perception of it. For false-premise\"\n",
    "    \" questions, the response must point out the presence of a false premise to\"\n",
    "    \" receive credit. For answers that involve names of entities (e.g.,\"\n",
    "    \" people), complete names or commonly recognized names are expected.\"\n",
    "    \" Regarding numerical answers, approximate numbers are generally not\"\n",
    "    \" accepted unless explicitly included in the ground-truth answers. A\"\n",
    "    \" response that contains any hallucination, no matter how minor, will not\"\n",
    "    \" receive credit. Furthermore, when the response indicates that the\"\n",
    "    \" information might be outdated, we accept it only if it is evident that\"\n",
    "    \" the knowledge has not changed (e.g., through common sense or well-known\"\n",
    "    \" facts).\"\n",
    ")\n",
    "\n",
    "demo_examples = [\n",
    "    {\n",
    "        \"question\": \"How old is the world's oldest verified living person?\",\n",
    "        \"correct_answers\": [\n",
    "            \"117 years old\",\n",
    "            \"117\",\n",
    "        ],\n",
    "        \"response\": (\n",
    "            f\"As of today {current_date}, the most up-to-date and relevant\"\n",
    "            \" information regarding this query is as follows. The oldest\"\n",
    "            \" verified living person is Maria Branyas Morera, who was born\"\n",
    "            \" on March 4, 1907, making her 117 years old.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. The primary answer in the response\"\n",
    "            \" (117 years old) is accurate, and all the provided information\"\n",
    "            \" is accurate and up-to-date. Thus, the response is credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"correct\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When did the UK adopt the Euro?\",\n",
    "        \"correct_answers\": [\"The United Kingdom has never adopted the Euro.\"],\n",
    "        \"response\": (\n",
    "            \"The UK has never adopted the Euro as its official currency. The\"\n",
    "            \" country has retained the British pound sterling (GBP) as its\"\n",
    "            \" currency.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"The question contains a false premise. The response debunks the\"\n",
    "            \" question's false premise (i.e., it points out that the UK has\"\n",
    "            \" never adopted the Euro), and all the provided information is\"\n",
    "            \" accurate and up-to-date. Thus, the response is credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"correct\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": (\n",
    "            \"How much longer will Brittney Griner spend in the Russian prison?\"\n",
    "        ),\n",
    "        \"correct_answers\": [\n",
    "            \"She was released in December 2022 as part of a prisoner swap.\"\n",
    "        ],\n",
    "        \"response\": (\n",
    "            \"I'm sorry, but I have no information to suggest that Brittney\"\n",
    "            \" Griner is currently in a Russian prison. As far as I know, she\"\n",
    "            \" is a professional basketball player who currently plays for the\"\n",
    "            \" Phoenix Mercury in the Women's National Basketball Association\"\n",
    "            \" (WNBA). If you have any more information or context, please let\"\n",
    "            \" me know and I'll do my best to help.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"The question contains a false premise. The response fails to\"\n",
    "            \" debunk the question's false premise by not explicitly stating\"\n",
    "            \" that Brittney Griner was already released. Additionally, the\"\n",
    "            \" response lacks a confident and definitive answer. Thus, the\"\n",
    "            \" response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": (\n",
    "            \"Which language has the third largest population by number of\"\n",
    "            \" native speakers?\"\n",
    "        ),\n",
    "        \"correct_answers\": [\"English\"],\n",
    "        \"response\": \"1. Mandarin 2. Spanish 3. English\",\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. While the correct answer (English) can\"\n",
    "            \" be inferred from the response, the response lacks an explicit\"\n",
    "            \" statement, which is not desirable.\"\n",
    "            \" Thus, the response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": (\n",
    "            \"Has Virginia Woolf's novel about the Ramsay family entered the\"\n",
    "            \" public domain in the United States?\"\n",
    "        ),\n",
    "        \"correct_answers\": [\"Yes\"],\n",
    "        \"response\": (\n",
    "            \"1927's To the Lighthouse is in the public domain in the United\"\n",
    "            \" States. The final answer is To the Lighthouse.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. While the primary information in the\"\n",
    "            \" response (1927's To the Lighthouse is in the public domain in\"\n",
    "            \" the United States) is accurate, the final answer when standing\"\n",
    "            ' alone is inaccurate (it should be \"Yes\" instead of \"To the'\n",
    "            ' Lighthouse\"). Thus, the response is not credited.'\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Is Tesla's stock price above $250 a share?\",\n",
    "        \"correct_answers\": [\"No\"],\n",
    "        \"response\": \"No, it isn't. The stock price is currently at $257.\",\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. While the primary answer in the\"\n",
    "            \" response (No) is accurate, the additional information\"\n",
    "            \" contradicts the primary answer ($257 is above $250). Thus,\"\n",
    "            \" the response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When did Lionel Messi win his second FIFA World Cup?\",\n",
    "        \"correct_answers\": [\n",
    "            \"Messi has not won his second FIFA World Cup.\",\n",
    "            \"Messi has won only one FIFA World Cup.\",\n",
    "        ],\n",
    "        \"response\": (\n",
    "            \"This question does not contain a valid premise. Lionel Messi has\"\n",
    "            \" never won a FIFA World Cup.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"The question contains a false premise. While the response\"\n",
    "            \" debunks the question's false premise (i.e., it suggests that\"\n",
    "            \" Messi has not won his second FIFA World Cup), it inaccurately\"\n",
    "            \" shapes one's knowledge perception of the primary answer, as\"\n",
    "            \" Messi has already won one FIFA World Cup. Thus,\"\n",
    "            \" the response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": (\n",
    "            \"How many times has Jeremy Corbyn served as Prime Minister of\"\n",
    "            \" the UK?\"\n",
    "        ),\n",
    "        \"correct_answers\": [\n",
    "            \"Corbyn has never served as Prime Minister of the UK.\"\n",
    "        ],\n",
    "        \"response\": \"Zero\",\n",
    "        \"comment\": (\n",
    "            \"The question contains a false premise. The response lacks a\"\n",
    "            \" rebuttal, which is always required for false-premise questions.\"\n",
    "            \" Thus, the response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who founded Amazon?\",\n",
    "        \"correct_answers\": [\"Jeff Bezos\", \"Jeffrey Preston Bezos\"],\n",
    "        \"response\": \"100% of people said Jeff.\",\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. Given that the question involves a\"\n",
    "            \" person's name, a complete name or commonly recognized name is\"\n",
    "            ' expected (\"Jeff Bezos\" instead of \"Jeff\"). Thus, the response'\n",
    "            \" is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How many books has Colleen Hoover published?\",\n",
    "        \"correct_answers\": [\"26 books\", \"26\"],\n",
    "        \"response\": (\n",
    "            f\"As of today {current_date}, the most up-to-date and relevant\"\n",
    "            \" information regarding this query is as follows. Colleen Hoover\"\n",
    "            \" has published over 20 books and novellas.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. For a numerical answer, an approximate\"\n",
    "            \" value (over 20 books) is generally not accepted unless\"\n",
    "            \" explicitly included in the correct answers. Thus, the response\"\n",
    "            \" is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the largest model size of GPT-3?\",\n",
    "        \"correct_answers\": [\n",
    "            \"175 billion parameters\",\n",
    "            \"175B parameters\",\n",
    "            \"175 billion\",\n",
    "            \"175B\",\n",
    "        ],\n",
    "        \"response\": (\n",
    "            \"175 billion parameters. It is a 175 billion parameter, 1.37TB,\"\n",
    "            \" 137B token, 137B wordpiece, 137B BPE, 137B byte pair encoding,\"\n",
    "            \" 137B BPE token, 137B BPE wordpiece, 137B BPE token, 137B BPE\"\n",
    "            \" wordpiece\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. While the primary answer in the response\"\n",
    "            \" (175 billion parameters) is accurate, the response is ill-formed\"\n",
    "            \" and contains repetitions. Thus, the response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How many member nations exist in the United Nations?\",\n",
    "        \"correct_answers\": [\n",
    "            \"193\",\n",
    "            \"193 members\",\n",
    "            \"193 member states\",\n",
    "            \"193 sovereign states\",\n",
    "        ],\n",
    "        \"response\": (\n",
    "            \"As of my last update in April 2023, there were 193\"\n",
    "            \" member nations in the United Nations. Please note that this\"\n",
    "            \" information may have changed since then, so it is advisable to\"\n",
    "            \" check the latest information from the United Nations or other\"\n",
    "            \" reliable sources.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. The primary answer in the response (193\"\n",
    "            \" member nations) is accurate. However, the response fails to\"\n",
    "            \" provide a current answer and instead relies on outdated \"\n",
    "            \" information without confirming its continued accuracy. Thus, the\"\n",
    "            \" response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": (\n",
    "            \"Who won the biggest single-tournament payday in tennis history?\"\n",
    "        ),\n",
    "        \"correct_answers\": [\"Novak Djokovic\"],\n",
    "        \"response\": (\n",
    "            \"The biggest single-tournament payday in tennis history was won\"\n",
    "            \" by Novak Djokovic at the 2021 Australian Open. He received a\"\n",
    "            \" prize money of AUD 2.75 million (approximately USD 2.1 million)\"\n",
    "            \" for winning the men's singles title.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. While the primary answer in the\"\n",
    "            \" response (Novak Djokovic) is accurate, the additional provided\"\n",
    "            \" information is outdated or hallucinated (by winning the 2022\"\n",
    "            \" ATP Finals title, Djokovic earned a record $4,740,300, the\"\n",
    "            \" highest payout for a tournament winner in tennis). Thus, the\"\n",
    "            \" response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": (\n",
    "            \"Who was the first football player to score one hundred\"\n",
    "            \" international goals?\"\n",
    "        ),\n",
    "        \"correct_answers\": [\"Ali Daei\"],\n",
    "        \"response\": (\n",
    "            \"As of my last update in April 2023, Ali Daei, a\"\n",
    "            \" former Iranian footballer, was the first player to score 100\"\n",
    "            \" international goals.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. The primary answer in the response\"\n",
    "            \" (Ali Daei) is accurate. Although the knowledge cutoff date is\"\n",
    "            \" in 2021, this fact is timeless because Ali Daei was the initial\"\n",
    "            \" achiever of this milestone, and he will always remain the first).\"\n",
    "            \" Thus, the response is credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"correct\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": (\n",
    "            \"Who is the latest winner of the Formula 1 world championship?\"\n",
    "        ),\n",
    "        \"correct_answers\": [\"Max Verstappen\", \"Max Emilian Verstappen\"],\n",
    "        \"response\": (\n",
    "            \"Max Verstappen won the 2023 Formula 1 world championship. He is\"\n",
    "            \" a Dutch racing driver who competes in Formula One for Red Bull\"\n",
    "            \" Racing. He is the son of former Formula One driver Jos\"\n",
    "            \" Verstappen. Verstappen started his racing career in karting at\"\n",
    "            \" the age of seven. He won the 2013 European Karting Championship\"\n",
    "            \" and the 2013 World Karting Championship. In 2014, he moved to\"\n",
    "            \" single-seater racing, competing in the European Formula Three\"\n",
    "            \" Championship. He won the championship in his first season,\"\n",
    "            \" becoming the first driver to do so since Nico Rosberg in 2005.\"\n",
    "            \" In 2015, Verstappen moved to Formula One, driving for Toro\"\n",
    "            \" Rosso. He became the youngest driver to compete in Formula One\"\n",
    "            \" at the age of 17. He scored his first points in Formula One at\"\n",
    "            \" the 2015 Hungarian GrandPrix. In 2016, Verstappen moved to Red\"\n",
    "            \" Bull Racing. He won his first race at the 2016 Spanish Grand\"\n",
    "            \" Prix. He became the youngest driver to win a race in Formula\"\n",
    "            \" One at theage of 18. Verstappen finished the 2016 season in\"\n",
    "            \" third place in the drivers' championship. In 2017, Verstappen\"\n",
    "            \" won four races and finished the season in second place in the\"\n",
    "            \" drivers' championship. In 2018, Verstappen won seven races and\"\n",
    "            \" finished the season in second place in the drivers'\"\n",
    "            \" championship. In 2019, Verstappen won nine races and finished\"\n",
    "            \" the season in first place in the drivers' championship. He is\"\n",
    "            \" the first Dutch driver to win the Formula One world\"\n",
    "            \" championship.\"\n",
    "        ),\n",
    "        \"comment\": (\n",
    "            \"This is a valid question. While the primary answer in the\"\n",
    "            \" response (Max Verstappen) is accurate, the response contains\"\n",
    "            \" several instances of hallucinated information (e.g., Max\"\n",
    "            \" Verstappen did not win the Formula Three European Championship\"\n",
    "            \" in 2014). Thus, the response is not credited.\"\n",
    "        ),\n",
    "        \"evaluation\": \"incorrect\",\n",
    "    },\n",
    "]\n",
    "\n",
    "demo_questions = [ex[\"question\"] for ex in demo_examples]\n",
    "\n",
    "demo_evaluation_template = (\n",
    "    \"\\ncorrect answer(s): {correct_answers}\"\n",
    "    \"\\nresponse: {response}\"\n",
    "    \"\\ncomment: {comment}\"\n",
    "    \"\\nevaluation: {evaluation}\"\n",
    ")\n",
    "evaluation_template = (\n",
    "    \"\\ncorrect answer(s): {correct_answers}\"\n",
    "    \"\\nresponse: {response}\"\n",
    "    \"\\ncomment: \"\n",
    ")\n",
    "\n",
    "demo_evaluations = []\n",
    "for ex in demo_examples:\n",
    "  demo_evaluation = demo_evaluation_template.format(\n",
    "      question=ex[\"question\"],\n",
    "      correct_answers=' | '.join(ex[\"correct_answers\"]),\n",
    "      response=ex[\"response\"],\n",
    "      comment=ex[\"comment\"],\n",
    "      evaluation=ex[\"evaluation\"],\n",
    "  )\n",
    "  demo_evaluations.append(demo_evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "fAcuImw5EP5T"
   },
   "outputs": [],
   "source": [
    "#@title Function calling for FreshEval\n",
    "\n",
    "\n",
    "def call_fresheval(model, prefix, question, response, correct_answers,\n",
    "                   evaluation):\n",
    "  temperature = 0.0\n",
    "  max_tokens = 256\n",
    "  chat_completions = True\n",
    "\n",
    "  if model.startswith('gpt-4'):\n",
    "    num_organic_results = 15\n",
    "    num_related_questions = 3\n",
    "    num_questions_and_answers = 3\n",
    "    num_retrieved_evidences = 15\n",
    "  else:\n",
    "    num_organic_results = 15\n",
    "    num_related_questions = 2\n",
    "    num_questions_and_answers = 2\n",
    "    num_retrieved_evidences = 5\n",
    "\n",
    "  # Generate prompts for demo examples\n",
    "  demo_prompts = []\n",
    "  for q, e in zip(demo_questions, demo_evaluations):\n",
    "      demo_prompts.append(f'\\n\\n\\nquestion: {q}{e}')\n",
    "\n",
    "  fresheval_demo = ''.join(demo_prompts).strip()\n",
    "\n",
    "  fresheval_question = f'\\n\\n\\nquestion: {question}{evaluation}'\n",
    "\n",
    "  fresh_eval = prefix + '\\n\\n\\n' + fresheval_demo + fresheval_question\n",
    "  answer = call_llm_api(\n",
    "      fresh_eval, model, temperature, max_tokens, chat_completions\n",
    "  )\n",
    "\n",
    "  return answer\n",
    "\n",
    "\n",
    "def extract_ratings(response):\n",
    "  evaluation = None\n",
    "  for line in response.split('\\n'):\n",
    "    if 'evaluation: ' in line:\n",
    "      evaluation = line.split(' ')[-1]\n",
    "      if evaluation not in ['correct', 'incorrect']:\n",
    "        return False, {'rating': None}\n",
    "      if evaluation == 'incorrect':\n",
    "        evaluation = 'FALSE'\n",
    "      else:\n",
    "        evaluation = 'TRUE'\n",
    "  if evaluation is None:\n",
    "    if 'Thus, the response is credited.' in response:\n",
    "      evaluation = 'TRUE'\n",
    "    elif 'Thus, the response is not credited.' in response:\n",
    "      evaluation = 'FALSE'\n",
    "    else:\n",
    "      return False, {'rating': None}\n",
    "  return True, {'rating': evaluation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>question</th>\n",
       "      <th>model_response</th>\n",
       "      <th>rating</th>\n",
       "      <th>explanation</th>\n",
       "      <th>effective_year</th>\n",
       "      <th>next_review</th>\n",
       "      <th>false_premise</th>\n",
       "      <th>num_hops</th>\n",
       "      <th>fact_type</th>\n",
       "      <th>...</th>\n",
       "      <th>answer_1</th>\n",
       "      <th>answer_2</th>\n",
       "      <th>answer_3</th>\n",
       "      <th>answer_4</th>\n",
       "      <th>answer_5</th>\n",
       "      <th>answer_6</th>\n",
       "      <th>answer_7</th>\n",
       "      <th>answer_8</th>\n",
       "      <th>answer_9</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST</td>\n",
       "      <td>What is the name of the first animal to land o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>before 2022</td>\n",
       "      <td>occasionally</td>\n",
       "      <td>True</td>\n",
       "      <td>one-hop</td>\n",
       "      <td>slow-changing</td>\n",
       "      <td>...</td>\n",
       "      <td>Since humans are animals, one could say Neil A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST</td>\n",
       "      <td>What is the name of Leonardo DiCaprio's third ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>before 2022</td>\n",
       "      <td>occasionally</td>\n",
       "      <td>True</td>\n",
       "      <td>one-hop</td>\n",
       "      <td>slow-changing</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST</td>\n",
       "      <td>What year did the first human land on Mars?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>before 2022</td>\n",
       "      <td>occasionally</td>\n",
       "      <td>True</td>\n",
       "      <td>one-hop</td>\n",
       "      <td>slow-changing</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST</td>\n",
       "      <td>What was the name of the Zodiac killer?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>before 2022</td>\n",
       "      <td>occasionally</td>\n",
       "      <td>True</td>\n",
       "      <td>one-hop</td>\n",
       "      <td>slow-changing</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST</td>\n",
       "      <td>Why are all quickly verifiable problems also q...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>before 2022</td>\n",
       "      <td>occasionally</td>\n",
       "      <td>True</td>\n",
       "      <td>one-hop</td>\n",
       "      <td>slow-changing</td>\n",
       "      <td>...</td>\n",
       "      <td>Whether all quickly verifiable problems are al...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   split                                           question  model_response  \\\n",
       "id                                                                            \n",
       "0   TEST  What is the name of the first animal to land o...             NaN   \n",
       "1   TEST  What is the name of Leonardo DiCaprio's third ...             NaN   \n",
       "2   TEST        What year did the first human land on Mars?             NaN   \n",
       "3   TEST            What was the name of the Zodiac killer?             NaN   \n",
       "4   TEST  Why are all quickly verifiable problems also q...             NaN   \n",
       "\n",
       "    rating  explanation effective_year   next_review  false_premise num_hops  \\\n",
       "id                                                                             \n",
       "0      NaN          NaN    before 2022  occasionally           True  one-hop   \n",
       "1      NaN          NaN    before 2022  occasionally           True  one-hop   \n",
       "2      NaN          NaN    before 2022  occasionally           True  one-hop   \n",
       "3      NaN          NaN    before 2022  occasionally           True  one-hop   \n",
       "4      NaN          NaN    before 2022  occasionally           True  one-hop   \n",
       "\n",
       "        fact_type  ...                                           answer_1  \\\n",
       "id                 ...                                                      \n",
       "0   slow-changing  ...  Since humans are animals, one could say Neil A...   \n",
       "1   slow-changing  ...                                                NaN   \n",
       "2   slow-changing  ...                                                NaN   \n",
       "3   slow-changing  ...                                                NaN   \n",
       "4   slow-changing  ...  Whether all quickly verifiable problems are al...   \n",
       "\n",
       "   answer_2 answer_3 answer_4 answer_5 answer_6 answer_7 answer_8 answer_9  \\\n",
       "id                                                                           \n",
       "0       NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "1       NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2       NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "3       NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "4       NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "   note  \n",
       "id       \n",
       "0   NaN  \n",
       "1   NaN  \n",
       "2   NaN  \n",
       "3   NaN  \n",
       "4   NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating example id=0...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 2: expected str instance, float found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m   correct_answers \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)]\n\u001b[1;32m     16\u001b[0m   correct_answers \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m correct_answers \u001b[38;5;28;01mif\u001b[39;00m x]\n\u001b[1;32m     18\u001b[0m   evaluation \u001b[38;5;241m=\u001b[39m evaluation_template\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m---> 19\u001b[0m     correct_answers\u001b[38;5;241m=\u001b[39m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m | \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrect_answers\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     20\u001b[0m     response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m   fresheval \u001b[38;5;241m=\u001b[39m call_fresheval(\n\u001b[1;32m     24\u001b[0m     model_name,\n\u001b[1;32m     25\u001b[0m     prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     evaluation,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m   is_valid_eval, \u001b[38;5;28meval\u001b[39m \u001b[38;5;241m=\u001b[39m extract_ratings(fresheval)\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 2: expected str instance, float found"
     ]
    }
   ],
   "source": [
    "num_evals = 300\n",
    "df = pd.read_excel('fresheval_strict.xlsx', skiprows=2, index_col=0)[:num_evals]\n",
    "\n",
    "#@title FreshEval\n",
    "\n",
    "# @markdown ---\n",
    "model_name = \"gpt-4-1106-preview\" #@param [\"gpt-4-0125-preview\", \"gpt-4-turbo-preview\", \"gpt-4-1106-preview\", \"gpt-4\", \"gpt-4-0613\", \"gpt-4-32k\", \"gpt-4-32k-0613\", \"gpt-3.5-turbo-1106\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\", \"gpt-3.5-turbo-instruct\", \"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-16k-0613\", \"gpt-3.5-turbo-0301\"]\n",
    "evaluation_spreadsheet_name = \"fresheval_strict\" # @param {type:\"string\"}\n",
    "\n",
    "freshevals = []\n",
    "for index, row in df.iterrows():\n",
    "    print(f'Evaluating example id={index}...')\n",
    "    question = row['question']\n",
    "    response = row['model_response']\n",
    "    correct_answers = [row[f'answer_{i}'] for i in range(10)]\n",
    "    correct_answers = [x for x in correct_answers if x]\n",
    "\n",
    "    evaluation = evaluation_template.format(\n",
    "      correct_answers=' | '.join(correct_answers),\n",
    "      response=response,\n",
    "  )\n",
    "\n",
    "    fresheval = call_fresheval(\n",
    "      model_name,\n",
    "      prefix,\n",
    "      question,\n",
    "      response,\n",
    "      correct_answers,\n",
    "      evaluation,\n",
    "  )\n",
    "    is_valid_eval, eval = extract_ratings(fresheval)\n",
    "    if is_valid_eval:\n",
    "        print('Done')\n",
    "\n",
    "    while not is_valid_eval:\n",
    "        print('Invalid evaluation, reevaluating...')\n",
    "        fresheval = call_fresheval(\n",
    "        model_name,\n",
    "        prefix,\n",
    "        question,\n",
    "        response,\n",
    "        correct_answers,\n",
    "        evaluation,\n",
    "    )\n",
    "        is_valid_eval, eval = extract_ratings(fresheval)\n",
    "        if is_valid_eval:\n",
    "            print('Done')\n",
    "    freshevals.append({'rating': eval['rating'], 'explanation': fresheval})\n",
    "\n",
    "df_eval = pd.DataFrame(freshevals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "HCdw8F-MZZlO",
    "outputId": "ae1b6659-ec51-46c9-de2c-c2d32da89235"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df_eval[:num_evals]\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"samples\": [\n          \"TRUE\",\n          \"FALSE\"\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"explanation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"The question contains a false premise. The response fails to debunk the question's false premise by incorrectly stating that Mark Cuban sold the Dallas Mavericks in February 2018. Thus, the response is not credited.\\nevaluation: incorrect\",\n          \"The question contains a false premise. The response correctly debunks the question's false premise (i.e., it points out that Leonardo DiCaprio does not have any children). However, the response fails to provide a current answer and instead relies on outdated information without confirming its continued accuracy. Thus, the response is not credited.\\nevaluation: incorrect\"\n        ],\n        \"num_unique_values\": 10,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-9cadc527-f3a9-40e2-a331-f0b2a0f35b15\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FALSE</td>\n",
       "      <td>The question contains a false premise. The res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FALSE</td>\n",
       "      <td>The question contains a false premise. The res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRUE</td>\n",
       "      <td>The question contains a false premise. The res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRUE</td>\n",
       "      <td>The question contains a false premise. The res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FALSE</td>\n",
       "      <td>The question is based on a common misconceptio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TRUE</td>\n",
       "      <td>The question contains a false premise. The res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FALSE</td>\n",
       "      <td>The question contains a false premise. The res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FALSE</td>\n",
       "      <td>The question contains a false premise. The res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FALSE</td>\n",
       "      <td>The question contains a false premise. The res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FALSE</td>\n",
       "      <td>The question contains a false premise. The res...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9cadc527-f3a9-40e2-a331-f0b2a0f35b15')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-9cadc527-f3a9-40e2-a331-f0b2a0f35b15 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-9cadc527-f3a9-40e2-a331-f0b2a0f35b15');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-a7f65875-8bfe-4aa3-bc80-a7d38e299e1b\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a7f65875-8bfe-4aa3-bc80-a7d38e299e1b')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-a7f65875-8bfe-4aa3-bc80-a7d38e299e1b button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "  rating                                        explanation\n",
       "0  FALSE  The question contains a false premise. The res...\n",
       "1  FALSE  The question contains a false premise. The res...\n",
       "2   TRUE  The question contains a false premise. The res...\n",
       "3   TRUE  The question contains a false premise. The res...\n",
       "4  FALSE  The question is based on a common misconceptio...\n",
       "5   TRUE  The question contains a false premise. The res...\n",
       "6  FALSE  The question contains a false premise. The res...\n",
       "7  FALSE  The question contains a false premise. The res...\n",
       "8  FALSE  The question contains a false premise. The res...\n",
       "9  FALSE  The question contains a false premise. The res..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval[:num_evals]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
